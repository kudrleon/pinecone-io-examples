{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building RAG Anwer Generator with LangChain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we'll work on building an AI answer generator engine from start-to-finish. We will be using LangChain, OpenAI, and Pinecone vector DB, to build the engine capable of learning from the external world using **R**etrieval **A**ugmented **G**eneration (RAG).\n",
    "\n",
    "We will be using a set of information files from a specified folder and another file with a list of questions. Each question will be answered independently in it's own context. The answers will be written as a CSV file with some additional information about where they were found in the provided files. The use case assumed in the example, and reflected in some specific promts, is answering a company security assessment questionnaire based on that company's policy and procedure documents. \n",
    "\n",
    "The example assumes that the documents are be prepared with blocks preceeded by a line with the block name starting with ##. The blocks themselves will be broken into chanks to make embedding and processing possible.\n",
    "\n",
    "By the end of the example we'll have a functioning answer generator using RAG pipeline.\n",
    "\n",
    "### Before you begin\n",
    "\n",
    "You'll need to get an [OpenAI API key](https://platform.openai.com/account/api-keys) and [Pinecone API key](https://app.pinecone.io)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start building our chatbot, we need to install some Python libraries. Here's a brief overview of what each library does:\n",
    "\n",
    "- **langchain**: This is a library for GenAI. We'll use it to chain together different language models and components for our chatbot.\n",
    "- **openai**: This is the official OpenAI Python client. We'll use it to interact with the OpenAI API and generate responses for our chatbot.\n",
    "- **pinecone-client**: This is the official Pinecone Python client. We'll use it to interact with the Pinecone API and store our chatbot's knowledge base in a vector database.\n",
    "- **python-dotenv**: Read .env file that contains encironment variables\n",
    "\n",
    "You can install these libraries using pip like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU \\\n",
    "    langchain \\\n",
    "    openai \\\n",
    "    pinecone-client \\\n",
    "    tiktoken \\\n",
    "    python-dotenv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Open AI "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be relying heavily on the LangChain library to bring together the different components needed for our chatbot. To begin, we'll create a simple chatbot without any retrieval augmentation. We do this by initializing a `ChatOpenAI` object. For this we do need an [OpenAI API key](https://platform.openai.com/account/api-keys)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "#from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "# Load the environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "if not os.getenv(\"OPENAI_API_KEY\"):\n",
    "    print(\"OPENAI_API_KEY Not Defined\");\n",
    "\n",
    "open_api_key = os.getenv(\"OPENAI_API_KEY\", \"OPENAI_API_KEY Not Defined\")\n",
    "\n",
    "#chat = ChatOpenAI(\n",
    "#    openai_api_key=open_api_key,\n",
    "#    model='gpt-3.5-turbo'\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.0 Read Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    }
   ],
   "source": [
    "def read_files_from_folder(folder_path):\n",
    "    files_data = []\n",
    "\n",
    "    # List all files in the given folder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "        # Check if it's a file and not a directory\n",
    "        if os.path.isfile(file_path):\n",
    "            # Open and read the file\n",
    "            with open(file_path, 'r') as file:\n",
    "                file_text = file.read()\n",
    "                files_data.append({'file_name': file_name, 'file_text': file_text})\n",
    "\n",
    "    return files_data\n",
    "\n",
    "# Example usage\n",
    "folder_path = \"/Users/leo/text\"\n",
    "files = read_files_from_folder(folder_path)\n",
    "\n",
    "print(len(files))\n",
    "#for file in files:\n",
    "#    print(file['file_name'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3.1 Split Text Into Sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the files into sections. Section name has format ## ***\\n. The text before the first section name (header) may or may not be igrnored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "IGNORE_HEADER = True\n",
    "\n",
    "def split_into_blocks(text):\n",
    "    # Regular expression pattern to find the block markers, assuming they end with a newline\n",
    "    pattern = r'##\\s*(.*?)\\n'\n",
    "\n",
    "    text = text.strip()\n",
    "    if not IGNORE_HEADER and not text.startswith('##'):\n",
    "        text = '## FILE HEADER\\n' + text\n",
    "    \n",
    "    # Split the text based on the pattern\n",
    "    parts = re.split(pattern, text)\n",
    "\n",
    "    # First part is always before the first marker, which we can ignore\n",
    "    parts = parts[1:]\n",
    "\n",
    "    # Create a list of dictionaries from the split parts\n",
    "    # Odd indexed elements are section names, even indexed elements are section texts    \n",
    "    sections = [{'section_name': name, 'section_text': text} for name, text in zip(parts[0::2], parts[1::2])]\n",
    "\n",
    "    return sections\n",
    "\n",
    "# Example usage\n",
    "test_text = \"\"\"\n",
    "Header of the file that may or may not be ignored depending on IGNORE_HEADER flag\n",
    "## Introduction\n",
    "This is the introduction section.\n",
    "It has multiple lines, etc. '\n",
    "## Methodology\n",
    "Here we describe our methodology.\n",
    "## Results\n",
    "Here are the results.\n",
    "\"\"\"\n",
    "#test_result = split_into_blocks(test_text)\n",
    "#test_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split files into sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split blocks into chunks if required. Each chunk will become a section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "def get_text_chunks(text):\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        separator=\"\\n\",\n",
    "        chunk_size=5000,\n",
    "        chunk_overlap=400,\n",
    "        length_function=len\n",
    "    )\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    return chunks\n",
    "\n",
    "def process_files(files):\n",
    "    processed_data = []\n",
    "    for file in files:\n",
    "        blocks = split_into_blocks(file['file_text'])\n",
    "        for block in blocks:\n",
    "            #Split text into chunks\n",
    "            chunks = get_text_chunks(block['section_text'])\n",
    "            #======================\n",
    "            chunk_index = 0;\n",
    "            for chunk in chunks:\n",
    "                chunk_index += 1\n",
    "                processed_data.append({\n",
    "                    'file_name': file['file_name'],\n",
    "                    'section_name': block['section_name'] + ' #' + str(chunk_index),\n",
    "                    'section_text': chunk\n",
    "                })\n",
    "    return processed_data\n",
    "\n",
    "def sort_files_by_name(files):\n",
    "    # Sorting the files by the 'file_name' key\n",
    "    return sorted(files, key=lambda x: float(x['file_name'].split('-')[0]))\n",
    "\n",
    "files = sort_files_by_name(files)\n",
    "\n",
    "sections = process_files(files)\n",
    "\n",
    "print(len(sections))\n",
    "#for section in sections:\n",
    "#    print(section['file_name'], section['section_name'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Building the Knowledge Base"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a file chunks that can serve as our answer engine knowledge base. Our next task is to transform the chunks into the knowledge base that our engine can use. To do this we must use an embedding model and vector database.\n",
    "\n",
    "We begin by initializing our connection to Pinecone, this requires a [free API key](https://app.pinecone.io)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "if not os.getenv(\"PINECONE_API_KEY\"):\n",
    "    print(\"PINECONE_API_KEY Not Defined\")\n",
    "    \n",
    "# initialize connection to pinecone (get API key at app.pinecone.io)\n",
    "api_key = os.getenv(\"PINECONE_API_KEY\", \"PINECONE_API_KEY Not Defined\")\n",
    "\n",
    "# configure client\n",
    "pc = Pinecone(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we setup our index specification, this allows us to define the cloud provider and region where we want to deploy our index. You can find a list of all [available providers and regions here](https://docs.pinecone.io/docs/projects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import ServerlessSpec\n",
    "\n",
    "spec = ServerlessSpec(\n",
    "    cloud=\"aws\", region=\"us-west-2\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we initialize the index. We will be using OpenAI's `text-embedding-ada-002` model for creating the embeddings, so we set the `dimension` to `1536`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {},\n",
       " 'total_vector_count': 0}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "index_name = 'llama-2-rag'\n",
    "existing_indexes = [\n",
    "    index_info[\"name\"] for index_info in pc.list_indexes()\n",
    "]\n",
    "\n",
    "# check if index already exists (it shouldn't if this is first time)\n",
    "if index_name not in existing_indexes:\n",
    "    # if does not exist, create index\n",
    "    pc.create_index(\n",
    "        index_name,\n",
    "        dimension=1536,  # dimensionality of ada 002\n",
    "        metric='dotproduct',\n",
    "        spec=spec\n",
    "    )\n",
    "    # wait for index to be initialized\n",
    "    while not pc.describe_index(index_name).status['ready']:\n",
    "        time.sleep(1)\n",
    "\n",
    "# connect to index\n",
    "index = pc.Index(index_name)\n",
    "time.sleep(1)\n",
    "# view index stats\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our index is now ready but it's empty. It is a vector index, so it needs vectors. As mentioned, to create these vector embeddings we will OpenAI's `text-embedding-ada-002` model — we can access it via LangChain like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leo/miniconda3/envs/prompt-eng/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.openai.OpenAIEmbeddings` was deprecated in langchain-community 0.1.0 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embed_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to embed and index all our our data! We do this by looping through our document sections, embedding, and inserting everything in batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb4efbc943ab49159b4ae52cae2ccd49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.auto import tqdm  # for progress bar\n",
    "import pandas as pd\n",
    "import uuid\n",
    "\n",
    "data = pd.DataFrame(sections);\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "for i in tqdm(range(0, len(data), batch_size)):\n",
    "    i_end = min(len(data), i+batch_size)\n",
    "    # get batch of data\n",
    "    batch = data.iloc[i:i_end]\n",
    "    # generate unique ids for each chunk\n",
    "    #ids = [f\"{x['doi']}-{x['chunk-id']}\" for i, x in batch.iterrows()]\n",
    "    ids = [f\"{uuid.uuid4()}\" for i, x in batch.iterrows()]\n",
    "    \n",
    "    # get text to embed\n",
    "    texts = [x['section_text'] for _, x in batch.iterrows()]\n",
    "    # embed text\n",
    "    embeds = embed_model.embed_documents(texts)\n",
    "    # get metadata to store in Pinecone\n",
    "    metadata = [\n",
    "        {'text': x['section_text'],\n",
    "         'source': x['file_name'],\n",
    "         'title': x['section_name']} for i, x in batch.iterrows()\n",
    "    ]\n",
    "    # add to Pinecone\n",
    "    index.upsert(vectors=zip(ids, embeds, metadata))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: **This is not immediately available!** We can check that the vector index has been populated using `describe_index_stats` like before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {'': {'vector_count': 96}},\n",
       " 'total_vector_count': 96}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieval Augmented Generation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've built a fully-fledged knowledge base. Now it's time to connect that knowledge base to our engine. To do that we'll be diving back into LangChain and reusing our template prompt from earlier."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use LangChain here we need to load the LangChain abstraction for a vector index, called a `vectorstore`. We pass in our vector `index` to initialize the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leo/miniconda3/envs/prompt-eng/lib/python3.12/site-packages/langchain_community/vectorstores/pinecone.py:75: UserWarning: Passing in `embedding` as a Callable is deprecated. Please pass in an Embeddings object instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "text_field = \"text\"  # the metadata field that contains our text\n",
    "\n",
    "# initialize the vector store object\n",
    "vectorstore = Pinecone(\n",
    "    index, embed_model.embed_query, text_field\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137\n"
     ]
    }
   ],
   "source": [
    "def read_questions(file_path):\n",
    "    lines = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            stripped_line = line.strip()\n",
    "            if stripped_line and not stripped_line.startswith('##'):\n",
    "                lines.append(stripped_line)\n",
    "    return lines\n",
    "\n",
    "file_path = folder_path = \"/Users/leo/question/questions.txt\"\n",
    "questions = read_questions(file_path)\n",
    "#print(questions)\n",
    "print(len(questions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer All Questions One by One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_prompt(query: str):\n",
    "    # get top 3 results from knowledge base\n",
    "    results = vectorstore.similarity_search(query, k=2)\n",
    "    \n",
    "    # get the text from the results\n",
    "    source_knowledge = \"\\n\".join([x.page_content for x in results])\n",
    "    metadata_ref = \" ### \".join([x.metadata['source'] + '; ' + x.metadata['title'] for x in results])\n",
    "    #print(metadata_ref)\n",
    "    # feed into an augmented prompt\n",
    "    augmented_prompt = f\"\"\"Using the contexts below, answer the query. If there is no answer, answer \"No answer in the provided files\".\n",
    "\n",
    "    Contexts:\n",
    "    {source_knowledge}\n",
    "\n",
    "    Query: {query}\"\"\"\n",
    "    return augmented_prompt, metadata_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_yes_no_string(text):\n",
    "    # Check if the string starts with the specified patterns\n",
    "    match = re.match(r'^(Yes,|Yes\\.|No,|No\\.|yes,|yes\\.)\\s*(.*)', text)\n",
    "\n",
    "    if match:\n",
    "        # If it matches, the first group is part 1, and the rest is part 2\n",
    "        part1 = match.group(1)\n",
    "        part2 = match.group(2).strip()\n",
    "    else:\n",
    "        # If it does not match, the first part is empty, and the second part is the whole string\n",
    "        part1 = 'N/A'\n",
    "        part2 = text.strip()\n",
    "\n",
    "    # Remove . or , from part1\n",
    "    part1 = part1.rstrip('.,')\n",
    "\n",
    "    # Capitalize only the first letter of part2, if it exists\n",
    "    if part1:\n",
    "        part1 = part1[0].upper() + part1[1:]\n",
    "    \n",
    "    if part2:\n",
    "        part2 = part2[0].upper() + part2[1:]\n",
    "\n",
    "    return part1, part2\n",
    "\n",
    "# Example usage\n",
    "#text = \"yes,    this is a sample text. All is well.        \"\n",
    "#part1, part2 = split_yes_no_string(text)\n",
    "#print(f\"Part 1: '{part1}'\")\n",
    "#print(f\"Part 2: '{part2}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8566d354ffa84c09b67a297c8e8270f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/137 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/leo/miniconda3/envs/prompt-eng/lib/python3.12/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "csv_data = []\n",
    "\n",
    "for question in tqdm(questions):\n",
    "    messages = [\n",
    "        SystemMessage(content=\"You are a compliance officer at a company answering vendor assessment questionnaire.\"),\n",
    "    ]\n",
    "\n",
    "    content, reference = augment_prompt(question)\n",
    "    \n",
    "    # create a new user prompt\n",
    "    prompt = HumanMessage(\n",
    "        content=content\n",
    "    )\n",
    "    # add to messages\n",
    "    messages.append(prompt)\n",
    "    \n",
    "    res = chat(messages)\n",
    "\n",
    "    response_content = res.content\n",
    "\n",
    "    yesNo, text_answer = split_yes_no_string(response_content)\n",
    "    # Append the question, response, and reference to csv_data\n",
    "    \n",
    "    csv_data.append([question, yesNo, text_answer, reference])\n",
    "    \n",
    "with open('/Users/leo/question/answers.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "    # Write the header\n",
    "    writer.writerow(['Question', 'Yes/No', 'Content', 'Reference'])\n",
    "    # Write the data\n",
    "    writer.writerows(csv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.delete_index(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
